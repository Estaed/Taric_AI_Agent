# Architecture & File Structures

This document outlines the proposed file and folder structures for the main projects:
1.  `lol_sim_env`: The Python-based simulated 2v2 LoL laning environment for RL training (uses MCP server for champion/game data)
2.  `taric_ai_agent`: The Taric AI agent, using IL from MCP-provided datasets and RL within `lol_sim_env`
3.  `lol_data_mcp_server`: Provides data for BOTH simulation environment AND IL training datasets

## Project 2: Taric AI Agent (`taric_ai_agent`)

**Goal:** To develop a Taric AI agent using **Imitation Learning (IL) from MCP-provided ready-to-train datasets** and **Reinforcement Learning (RL) within the `lol_sim_env`**. The MCP server provides sophisticated training data for IL and also supplies game data to the simulation environment.

**Updated Folder Structure (MCP-Enhanced):**

```
taric_ai_agent/
â”œâ”€â”€ taric_ai_agent/                    # Core Python package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents/                        # Agent model definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py              # Abstract base class for agents
â”‚   â”‚   â”œâ”€â”€ il_policy_network.py       # IL neural network (enhanced for MCP features)
â”‚   â”‚   â””â”€â”€ rl_agent_sb3.py           # RL agent setup using Stable Baselines3
â”‚   â”œâ”€â”€ mcp/                          # MCP integration (NEW - PRIMARY for IL)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mcp_client.py             # MCP protocol client for LoL Data server
â”‚   â”‚   â”œâ”€â”€ dataset_fetcher.py        # Fetch ready-to-train IL datasets via MCP tools
â”‚   â”‚   â”œâ”€â”€ data_converter.py         # Convert MCP data to training-ready format
â”‚   â”‚   â””â”€â”€ live_services.py          # Real-time MCP services for meta updates
â”‚   â”œâ”€â”€ il/                           # Imitation Learning (MCP-based, SIMPLIFIED)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset_processor.py      # Process MCP datasets for training
â”‚   â”‚   â”œâ”€â”€ data_validator.py         # Validate MCP data quality
â”‚   â”‚   â”œâ”€â”€ augmentation.py           # Data augmentation techniques
â”‚   â”‚   â””â”€â”€ il_dataset.py            # PyTorch Dataset class for MCP IL data
â”‚   â”œâ”€â”€ rl/                           # Reinforcement Learning (Sim Env based)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rl_trainer.py            # RL training in lol_sim_env
â”‚   â”‚   â”œâ”€â”€ reward_functions.py       # Custom reward functions for Taric
â”‚   â”‚   â””â”€â”€ env_interface.py          # Interface with lol_sim_env
â”‚   â”œâ”€â”€ training/                     # Training pipelines (NEW)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ il_trainer.py            # IL training loop using MCP data
â”‚   â”‚   â”œâ”€â”€ combined_trainer.py      # IL â†’ RL pipeline trainer
â”‚   â”‚   â”œâ”€â”€ metrics_tracker.py       # Training metrics and validation
â”‚   â”‚   â””â”€â”€ model_checkpointing.py   # Model saving/loading
â”‚   â”œâ”€â”€ simulation/                   # Simulation integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state_mapper.py          # Map between MCP data and sim env states
â”‚   â”‚   â”œâ”€â”€ action_mapper.py         # Map between IL actions and sim actions
â”‚   â”‚   â””â”€â”€ env_integration.py       # Integration utilities with lol_sim_env
â”‚   â”œâ”€â”€ evaluation/                   # Evaluation framework (NEW)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ il_evaluator.py          # Evaluate IL performance
â”‚   â”‚   â”œâ”€â”€ rl_evaluator.py          # Evaluate RL performance
â”‚   â”‚   â”œâ”€â”€ combined_evaluator.py    # Evaluate IL+RL pipeline
â”‚   â”‚   â”œâ”€â”€ metrics.py               # Performance metrics
â”‚   â”‚   â””â”€â”€ benchmarking.py          # Player comparison via MCP
â”‚   â”œâ”€â”€ common/                       # Shared utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_manager.py          # Dataset management and versioning
â”‚   â”‚   â”œâ”€â”€ config_manager.py        # Configuration management
â”‚   â”‚   â”œâ”€â”€ state_action_mapper.py   # Legacy mapping utilities
â”‚   â”‚   â””â”€â”€ utils.py                 # General utility functions
â”‚   â””â”€â”€ utils/                        # Additional utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                # Logging utilities
â”‚       â”œâ”€â”€ visualization.py         # Training visualizations
â”‚       â””â”€â”€ training_utils.py        # ML training utilities
â”œâ”€â”€ scripts/                          # Execution scripts
â”‚   â”œâ”€â”€ fetch_training_data.py       # Fetch IL datasets from MCP (NEW)
â”‚   â”œâ”€â”€ train_il_model.py           # Train IL model using MCP data (UPDATED)
â”‚   â”œâ”€â”€ train_rl_agent.py           # Train RL agent in lol_sim_env
â”‚   â”œâ”€â”€ train_combined.py           # IL â†’ RL combined training pipeline (NEW)
â”‚   â”œâ”€â”€ evaluate_agent.py           # Comprehensive agent evaluation (NEW)
â”‚   â”œâ”€â”€ benchmark_vs_players.py     # Compare against high-ELO players via MCP (NEW)
â”‚   â””â”€â”€ simulate_agent_performance.py # Test agent in simulation environment (NEW)
â”œâ”€â”€ configs/                         # Configuration files
â”‚   â”œâ”€â”€ mcp_config.yaml             # MCP server connection settings (NEW)
â”‚   â”œâ”€â”€ il_training_config.yaml     # IL training parameters (UPDATED)
â”‚   â”œâ”€â”€ rl_training_config.yaml     # RL training parameters
â”‚   â”œâ”€â”€ combined_training_config.yaml # IL â†’ RL pipeline config (NEW)
â”‚   â”œâ”€â”€ model_config.yaml           # Model architectures
â”‚   â”œâ”€â”€ evaluation_config.yaml      # Evaluation settings (NEW)
â”‚   â””â”€â”€ simulation_config.yaml      # Simulation environment settings (NEW)
â”œâ”€â”€ data/                           # Data storage
â”‚   â”œâ”€â”€ mcp_datasets/              # Downloaded MCP training datasets (NEW)
â”‚   â”‚   â”œâ”€â”€ taric_estaed_dataset_v1.pt    # Player-specific datasets
â”‚   â”‚   â”œâ”€â”€ taric_scenarios_v1.json       # Scenario-labeled data
â”‚   â”‚   â””â”€â”€ taric_meta_builds_v1.json     # Meta information
â”‚   â”œâ”€â”€ processed/                  # Processed training data
â”‚   â”‚   â”œâ”€â”€ il_training_data/       # IL training splits
â”‚   â”‚   â””â”€â”€ rl_interaction_data/    # RL environment interaction logs
â”‚   â”œâ”€â”€ evaluation_results/         # Performance evaluation results (NEW)
â”‚   â”‚   â”œâ”€â”€ il_performance/         # IL model evaluation
â”‚   â”‚   â”œâ”€â”€ rl_performance/         # RL agent evaluation
â”‚   â”‚   â””â”€â”€ combined_performance/   # IL+RL pipeline evaluation
â”‚   â””â”€â”€ cache/                      # Cached data and temporary files
â”œâ”€â”€ trained_models/                 # Model artifacts
â”‚   â”œâ”€â”€ il_models/                  # IL trained models
â”‚   â”‚   â”œâ”€â”€ taric_il_v1.pth        # Checkpoints from MCP data training
â”‚   â”‚   â””â”€â”€ taric_il_best.pth      # Best performing IL model
â”‚   â”œâ”€â”€ rl_models/                  # RL trained models
â”‚   â”‚   â”œâ”€â”€ taric_rl_v1.zip        # SB3 RL models
â”‚   â”‚   â””â”€â”€ taric_rl_best.zip      # Best performing RL model
â”‚   â”œâ”€â”€ combined_models/            # IL+RL combined models (NEW)
â”‚   â”‚   â””â”€â”€ taric_combined_v1.zip   # Best IL+RL pipeline model
â”‚   â””â”€â”€ checkpoints/                # Training checkpoints
â”œâ”€â”€ notebooks/                      # Analysis notebooks
â”‚   â”œâ”€â”€ mcp_data_exploration.ipynb  # Explore MCP datasets (NEW)
â”‚   â”œâ”€â”€ il_training_analysis.ipynb  # IL training performance analysis
â”‚   â”œâ”€â”€ rl_training_analysis.ipynb  # RL training performance analysis
â”‚   â”œâ”€â”€ player_comparison.ipynb     # Compare agent to high-ELO players (NEW)
â”‚   â”œâ”€â”€ simulation_testing.ipynb    # Test agent in simulation environment
â”‚   â””â”€â”€ combined_pipeline_analysis.ipynb # Analyze IL+RL pipeline (NEW)
â”œâ”€â”€ tests/                          # Test suite
â”‚   â”œâ”€â”€ test_mcp_integration.py     # Test MCP client functionality (NEW)
â”‚   â”œâ”€â”€ test_il_training.py         # Test IL training pipeline
â”‚   â”œâ”€â”€ test_rl_training.py         # Test RL training pipeline
â”‚   â”œâ”€â”€ test_simulation_integration.py # Test lol_sim_env integration
â”‚   â”œâ”€â”€ test_evaluation.py          # Test evaluation framework (NEW)
â”‚   â””â”€â”€ test_combined_pipeline.py   # Test IL+RL pipeline (NEW)
â”œâ”€â”€ requirements.txt                # Dependencies (mcp-client, torch, sb3, etc.)
â””â”€â”€ README.md                       # Project overview, MCP+Simulation setup
```

## Key Architecture Changes for MCP Integration

### ğŸ¯ **Dual Data Flow Architecture**

```
MCP Server (lol_data_mcp_server)
    â”œâ”€â”€ Provides to lol_sim_env: Champion stats, abilities, items, game mechanics
    â””â”€â”€ Provides to taric_ai_agent: Ready-to-train IL datasets, scenarios, player demos

IL Training Path (NEW):
MCP Datasets â†’ IL Model â†’ Performance Evaluation

RL Training Path (EXISTING + ENHANCED):
IL Model â†’ Initialize RL Agent â†’ Train in lol_sim_env â†’ Final Agent

Combined Pipeline:
MCP Data â†’ IL Training â†’ RL Training (initialized with IL) â†’ Expert Agent
```

### ğŸ”„ **MCP Integration Points**

1. **For Simulation Environment Data:**
   - Champion abilities and stats
   - Item information and build paths
   - Game mechanics and formulas
   - Meta builds and trends

2. **For IL Training Data:**
   - Ready-to-train state-action pairs
   - High-ELO player demonstrations
   - Scenario-labeled training data
   - Enhanced features (positioning, combat metrics)

### ğŸ“Š **Data Management Strategy**

- **MCP Datasets** (`data/mcp_datasets/`): Raw and processed datasets from MCP server
- **IL Training Data** (`data/processed/il_training_data/`): Prepared data for IL model training
- **RL Interaction Data** (`data/processed/rl_interaction_data/`): Environment interaction logs
- **Evaluation Results** (`data/evaluation_results/`): Performance analysis across both IL and RL

### ğŸ—ï¸ **Training Pipeline Architecture**

#### **Phase 1: IL Training (MCP-Powered)**
```python
# fetch_training_data.py
mcp_client.get_imitation_dataset(champion="Taric", players=["Estaed#TAR"])

# train_il_model.py  
il_trainer.train(mcp_dataset, validation_split=0.2)
```

#### **Phase 2: RL Training (Simulation-Powered)**
```python
# train_rl_agent.py
rl_agent = PPO(policy=il_model.to_sb3_policy(), env=lol_sim_env)
rl_agent.learn(total_timesteps=1000000)
```

#### **Phase 3: Combined Evaluation**
```python
# evaluate_agent.py
il_performance = evaluate_il_model(il_model, mcp_benchmark_data)
rl_performance = evaluate_rl_agent(rl_agent, lol_sim_env)
combined_score = evaluate_combined_pipeline(il_model, rl_agent)
```

## Integration with Other Projects

### **LoL Data MCP Server** (Data Provider)
- **Serves IL Training Data**: Ready-to-train datasets, player demonstrations, scenarios
- **Serves Simulation Data**: Champion stats, abilities, items for accurate simulation
- **Serves Meta Data**: Current builds, trends, performance benchmarks

### **LoL Simulation Environment** (RL Training Platform)
- **Receives Champion Data**: From MCP server for accurate game simulation
- **Provides RL Training**: Environment for reinforcement learning phase
- **Validates IL Models**: Test IL performance in simulated environment

### **Taric AI Agent** (Consumer & Trainer)
- **Consumes MCP Data**: For IL training and meta analysis
- **Consumes Simulation**: For RL training and evaluation
- **Produces Trained Agent**: High-performing Taric AI using both IL and RL

This architecture enables the agent to leverage the best of both approaches: sophisticated IL training from real expert data via MCP, and robust RL training in a controlled simulation environment.